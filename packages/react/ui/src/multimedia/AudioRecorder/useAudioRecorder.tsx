import { useCallback, useEffect, useReducer, useRef } from 'react';

import {
  Loader,
  Pause,
  PlayFilled,
  Record,
  RecordPause,
  Refresh,
  Restart,
  Save,
} from '@edifice-ui/icons';
import { WorkspaceElement, WorkspaceVisibility } from 'edifice-ts-client';
import { useTranslation } from 'react-i18next';

import { ToolbarItem } from '../../components';
import { useWorkspaceFile } from '../../core';

export type RecordState =
  | 'IDLE'
  | 'RECORDING'
  | 'PAUSED'
  | 'RECORDED'
  | 'SAVING'
  | 'SAVED';

export type PlayState = 'IDLE' | 'PLAYING' | 'PAUSED';

type AudioReducerState = {
  recordState: RecordState;
  playState: PlayState;
  isEncoding: boolean;

  // The audio recorder is based on the Web Audio API (https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
  // See also https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletNode for more information.
  micStream?: MediaStream;
  micStreamAudioSourceNode?: MediaStreamAudioSourceNode;
  audioWorkletNode?: AudioWorkletNode;
  audioContext?: AudioContext;

  // encoder and web socket for sending encoded audio to the backend
  encoderWorker?: Worker;

  recordTime: number;
  // max duration in s (3 minutes by default)
  maxDuration: number;
};

export default function useAudioRecorder(
  onSaveSuccess?: (resource: WorkspaceElement) => void,
  onUpdateRecord?: (audioUrl?: string) => void,
  hideSaveAction: boolean = false,
  visibility: WorkspaceVisibility = 'protected',
  application: string = 'media-library',
) {
  function audioReducer(
    state: AudioReducerState,
    action: {
      type: 'update';
      updatedState?: Partial<AudioReducerState>;
    },
  ): AudioReducerState {
    return { ...state, ...action.updatedState };
  }

  const [
    {
      recordState,
      playState,
      micStream,
      micStreamAudioSourceNode,
      audioWorkletNode,
      audioContext,
      encoderWorker,
      maxDuration,
      recordTime,
      isEncoding,
    },
    dispatch,
  ] = useReducer(audioReducer, {
    recordState: 'IDLE',
    isEncoding: false,
    playState: 'IDLE',
    recordTime: 0,
    maxDuration: 180, // max duration in s (3 minutes by default)
  });
  const audioNameRef = useRef<HTMLInputElement>(null);
  const audioRef = useRef<HTMLAudioElement>(null);
  const leftChannelRef = useRef<Float32Array[]>();
  const rightChannelRef = useRef<Float32Array[]>();

  const { t } = useTranslation();
  const { create } = useWorkspaceFile();

  const BUFFER_SIZE: number = 128; // https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor
  const DEFAULT_SAMPLE_RATE: number = 48000;

  // Init Web Socket to send audio chunks to backend
  useEffect(() => {
    const encoderWorker = new Worker('/infra/public/js/audioEncoder.js');
    dispatch({
      type: 'update',
      updatedState: { encoderWorker: encoderWorker },
    });
    encoderWorker.postMessage([
      'init',
      audioContext?.sampleRate || DEFAULT_SAMPLE_RATE,
    ]);

    return () => {
      closeAudioStream();
      encoderWorker.terminate();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  /**
   * Handle message received from the audio recorder processor.
   * Basically the Audio recorder processor returns the input channels that will be treated by the audioEncoder worker.
   * The audio recorder processor script is located here: /infra/public/js/audio-recorder-processor.js
   * The script follows the AudioWorkletProcessor API (https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor)
   * This method gets called for each block of 128 sample-frames.
   *
   * @param event is the input returned, containing leftChannel and rightChannel arrays.
   */
  const handleAudioWorkletNodeMessage = (event: MessageEvent) => {
    const leftChannel = (event.data.inputs as Float32Array[][])[0][0];
    let rightChannel = (event.data.inputs as Float32Array[][])[0][1];
    if (
      !rightChannel ||
      rightChannel.filter((data) => data !== undefined).length === 0
    ) {
      rightChannel = leftChannel;
    }
    leftChannelRef.current?.push(leftChannel);
    rightChannelRef.current?.push(rightChannel);
  };

  /**
   * Close opened audio stream and clean channels
   */
  const closeAudioStream = useCallback(() => {
    // Close audio stream
    micStream?.getTracks().forEach((track) => track.stop());
    micStreamAudioSourceNode?.disconnect();
    audioWorkletNode?.port.removeEventListener(
      'message',
      handleAudioWorkletNodeMessage,
    );
    audioWorkletNode?.port.close();
    audioWorkletNode?.disconnect();
    audioContext?.close();
  }, [audioContext, audioWorkletNode, micStream, micStreamAudioSourceNode]);

  const initRecording = async () => {
    // Request access to the user's microphone
    const micStream: MediaStream = await navigator.mediaDevices.getUserMedia({
      audio: true,
    });

    // Create the microphone stream
    const audioContext = new AudioContext({ sampleRate: DEFAULT_SAMPLE_RATE });
    const micStreamAudioSourceNode: MediaStreamAudioSourceNode =
      audioContext.createMediaStreamSource(micStream);

    // Create and connect AudioWorkletNode for processing the audio stream
    // https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor
    try {
      await audioContext.audioWorklet.addModule(
        '/infra/public/js/audio-recorder-processor.js',
      );
    } catch (err) {
      console.error(err);
    }

    const audioWorkletNode = new AudioWorkletNode(
      audioContext,
      'audio-recorder-processor',
    );

    // Message received from the audio recorder processor. cf handleAudioWorkletNodeMessage function.
    // Basically the Audio recorder processor returns the input channels that will be treated by the audioEncoder worker.
    audioWorkletNode.port.addEventListener(
      'message',
      handleAudioWorkletNodeMessage,
    );

    // Store the audio context and stream in the state, reset record time and set states
    dispatch({
      type: 'update',
      updatedState: {
        micStream,
        micStreamAudioSourceNode,
        audioContext,
        recordTime: 0,
        audioWorkletNode,
        recordState: 'RECORDING',
        playState: 'IDLE',
      },
    });

    // Clean audio channels
    rightChannelRef.current = [];
    leftChannelRef.current = [];

    audioWorkletNode.port.start();

    micStreamAudioSourceNode.connect(audioWorkletNode);
    audioWorkletNode.connect(audioContext.destination);
  };

  const handleRecord = useCallback(async () => {
    if (audioRef.current) {
      audioRef.current.currentTime = 0;
      audioRef.current.pause();
    }
    if (recordState === 'PAUSED') {
      // Resume recording
      dispatch({
        type: 'update',
        updatedState: { recordState: 'RECORDING', playState: 'IDLE' },
      });
      audioContext?.resume();
      onUpdateRecord?.(undefined);
    } else {
      // Start new recording
      initRecording();
    }
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [recordState, audioContext, audioRef]);

  const handleRecordPause = useCallback(() => {
    dispatch({
      type: 'update',
      updatedState: {
        recordState: 'PAUSED',
        playState: 'IDLE',
        isEncoding: true,
      },
    });
    audioContext?.suspend();
    if (encoderWorker && leftChannelRef.current && rightChannelRef.current) {
      // Encode audio
      encoderWorker.postMessage([
        'wav',
        rightChannelRef.current,
        leftChannelRef.current,
        rightChannelRef.current.length * BUFFER_SIZE,
      ]);
      encoderWorker.onmessage = (event: MessageEvent) => {
        const audioUrl = window.URL.createObjectURL(event.data);

        dispatch({
          type: 'update',
          updatedState: { isEncoding: false },
        });

        if (audioRef.current) {
          audioRef.current.src = audioUrl;
        }

        if (onUpdateRecord) {
          onUpdateRecord(audioUrl);
        }
      };
    }
  }, [audioContext, encoderWorker, onUpdateRecord]);

  const handlePlay = useCallback(() => {
    dispatch({ type: 'update', updatedState: { playState: 'PLAYING' } });
    if (audioRef?.current?.currentTime) {
      audioRef.current.play();
    } else {
      if (audioRef.current) {
        audioRef.current.play();
      }
    }
  }, [audioRef]);

  const handlePlayPause = useCallback(() => {
    audioRef?.current?.pause();
    dispatch({ type: 'update', updatedState: { playState: 'PAUSED' } });
  }, [audioRef]);

  const handleReset = useCallback(() => {
    closeAudioStream();

    dispatch({
      type: 'update',
      updatedState: {
        playState: 'IDLE',
        recordState: 'IDLE',
      },
    });

    // Clean channels
    rightChannelRef.current = [];
    leftChannelRef.current = [];

    if (onUpdateRecord) {
      onUpdateRecord(undefined);
    }

    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [closeAudioStream, onUpdateRecord]);

  const handleSave: () => Promise<WorkspaceElement | undefined> =
    useCallback(async () => {
      const audioName = audioNameRef.current?.value;
      if (!audioName) {
        console.error('Audio name is required');
        return;
      }
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current.currentTime = 0;
      }
      dispatch({
        type: 'update',
        updatedState: { recordState: 'SAVING', playState: 'IDLE' },
      });
      return new Promise<WorkspaceElement | undefined>((resolve, reject) => {
        try {
          if (
            encoderWorker &&
            leftChannelRef.current &&
            rightChannelRef.current
          ) {
            encoderWorker.postMessage([
              'mp3',
              rightChannelRef.current,
              leftChannelRef.current,
              rightChannelRef.current.length * BUFFER_SIZE,
            ]);

            encoderWorker.onmessage = async (event: MessageEvent) => {
              if (event.data instanceof Blob) {
                const file = new File([event.data], audioName!, {
                  type: 'audio/mp3',
                });

                const resourceAudio = await create(file, {
                  application,
                  visibility,
                });

                if (resourceAudio && onSaveSuccess) {
                  onSaveSuccess(resourceAudio);
                }

                dispatch({
                  type: 'update',
                  updatedState: { recordState: 'SAVED' },
                });

                resolve(resourceAudio);
              }
            };
          }
        } catch (error) {
          dispatch({
            type: 'update',
            updatedState: { playState: 'IDLE', recordState: 'IDLE' },
          });
          console.error('Error while saving', error);
          reject();
        }
      });
      // eslint-disable-next-line react-hooks/exhaustive-deps
    }, [recordState, onSaveSuccess, closeAudioStream]);

  const handlePlayStop = useCallback(() => {
    // Stop Playing the record
    if (audioRef?.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
    }
    dispatch({ type: 'update', updatedState: { playState: 'IDLE' } });
  }, [audioRef]);

  const handlePlayEnded = useCallback(() => {
    dispatch({ type: 'update', updatedState: { playState: 'PAUSED' } });
    if (audioRef.current) {
      audioRef.current.currentTime = 0;
    }
  }, [audioRef]);

  /**
   * Auto-stop recording when max allowed duration is reached.
   */
  useEffect(() => {
    if (recordState === 'RECORDING' && audioContext) {
      const timer = window.setInterval(
        // Compute exact elapsed time by diffing the start time.
        () => {
          dispatch({
            type: 'update',
            updatedState: {
              recordTime: audioContext.currentTime,
            },
          });

          if (audioContext.currentTime >= maxDuration) {
            handleRecordPause();
            window.clearInterval(timer);
          }
        },
        100,
      );

      return () => window.clearInterval(timer);
    }
  }, [audioContext, handleRecordPause, maxDuration, recordState]);

  const recordText =
    recordState === 'IDLE'
      ? t('bbm.audio.record.start')
      : t('bbm.audio.record.resume');

  const toolbarItems: ToolbarItem[] = [
    {
      type: 'icon',
      name: 'record',
      visibility: recordState === 'RECORDING' ? 'hide' : 'show',
      props: {
        icon: <Record />,
        color: 'danger',
        disabled:
          (recordState !== 'IDLE' && recordState !== 'PAUSED') ||
          recordTime >= maxDuration,
        onClick: handleRecord,
        'aria-label': recordText,
      },
      tooltip: recordText,
    },
    {
      type: 'icon',
      name: 'recordPause',
      visibility: recordState === 'RECORDING' ? 'show' : 'hide',
      props: {
        icon: <RecordPause />,
        color: 'danger',
        disabled: recordState !== 'RECORDING',
        onClick: handleRecordPause,
        'aria-label': t('bbm.audio.record.pause'),
      },
      tooltip: t('bbm.audio.record.pause'),
    },
    { type: 'divider' },
    {
      type: 'icon',
      name: 'encoding',
      visibility: isEncoding ? 'show' : 'hide',
      props: {
        icon: <Loader style={{ animation: 'loading 1s infinite' }} />,
        disabled: true,
      },
    },
    {
      type: 'icon',
      name: 'play',
      visibility: isEncoding || playState === 'PLAYING' ? 'hide' : 'show',
      props: {
        icon: <PlayFilled />,
        disabled:
          recordState !== 'RECORDED' &&
          recordState !== 'PAUSED' &&
          recordState !== 'SAVED',
        onClick: handlePlay,
        'aria-label': t('bbm.audio.play.start'),
      },
      tooltip: t('bbm.audio.play.start'),
    },
    {
      type: 'icon',
      name: 'playPause',
      visibility: !isEncoding && playState === 'PLAYING' ? 'show' : 'hide',
      props: {
        icon: <Pause />,
        onClick: handlePlayPause,
        'aria-label': t('bbm.audio.play.pause'),
      },
      tooltip: t('bbm.audio.play.pause'),
    },
    {
      type: 'icon',
      name: 'stop',
      props: {
        icon: <Restart />,
        disabled: playState !== 'PLAYING' && playState !== 'PAUSED',
        onClick: handlePlayStop,
        'aria-label': t('bbm.audio.play.stop'),
      },
      tooltip: t('bbm.audio.play.stop'),
    },
    { type: 'divider' },
    {
      type: 'icon',
      name: 'reset',
      props: {
        icon: <Refresh />,
        disabled:
          recordState !== 'RECORDED' &&
          recordState !== 'PAUSED' &&
          playState !== 'PLAYING' &&
          playState !== 'PAUSED',
        onClick: handleReset,
        'aria-label': t('bbm.audio.record.reset'),
      },
      tooltip: t('bbm.audio.record.reset'),
    },
    {
      type: 'icon',
      name: 'save',
      visibility: hideSaveAction ? 'hide' : 'show',
      props: {
        icon: <Save />,
        disabled:
          (recordState !== 'RECORDED' &&
            recordState !== 'PAUSED' &&
            playState !== 'PLAYING' &&
            playState !== 'PAUSED') ||
          recordState === 'SAVED' ||
          !audioNameRef.current?.value,
        onClick: handleSave,
        'aria-label': t('bbm.audio.record.save'),
      },
      tooltip: t('bbm.audio.record.save'),
    },
  ];

  return {
    recordState,
    playState,
    audioContext,
    recordTime: recordTime * 1000,
    maxDuration: maxDuration * 1000,
    audioRef,
    audioNameRef,
    toolbarItems,
    handlePlayEnded,
    handleSave,
  };
}
